{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae90d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e87f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"rawdata.csv\",encoding='ISO-8859-1')\n",
    "var_df = pd.read_csv(\"Varlist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d828bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1211 entries, 0 to 1210\n",
      "Columns: 166 entries, STARTDATE to DEMOGRAPHIC13\n",
      "dtypes: bool(1), float64(2), int64(5), object(158)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b8a2e",
   "metadata": {},
   "source": [
    "### Figuring out what the types for each feature is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ac44b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['STARTDATE', 'ENDDATE', 'FINISHED', 'RECORDEDDATE', 'DISTRIBUTIONCHANNEL', 'USERLANGUAGE', 'ARM', 'CONSENT', 'COMPREHENSION1_ARM1', 'COMPREHENSION1_ARM2', 'COMPREHENSION2_ARM1', 'COMPREHENSION2_ARM2', 'HR1', 'HR2_1', 'HR2_2', 'HR2_3', 'HR2_4', 'HR2_5', 'HR3', 'HR4', 'HR5', 'HR6', 'HR7_1', 'HR7_2', 'HR7_3', 'HR7_4', 'HR8', 'HR10', 'HR11', 'HR11_6_TEXT', 'HR12_1', 'HR12_2', 'HR12_3', 'HR12_4', 'HR12_5', 'HR13', 'HR14', 'HR14_10_TEXT', 'COVID_HR1', 'COVID_HR2', 'COVID_HR2_11_TEXT', 'COVID_HR3', 'STIMULUS1', 'STIMULUS2', 'STIMULUS3', 'STIMULUS4', 'STIMULUS5', 'STIMULUS5_8_TEXT', 'STIMULUS6', 'STIMULUS6_7_TEXT', 'POLICY1', 'POLICY2', 'POLICY3', 'POLICY4', 'POLICY5', 'POLICY5_13_TEXT', 'POLICY6', 'POLICY6_10_TEXT', 'PERSONAL1', 'PERSONAL2', 'PERSONAL3', 'PERSONAL4', 'PERSONAL5', 'PERSONAL6', 'PERSONAL6_13_TEXT', 'PERSONAL7', 'PERSONAL7_11_TEXT', 'WORRY_1', 'WORRY_2', 'WORRY_3', 'WORRY_4', 'WORRY_5', 'WORRY_6', 'WORRY_7', 'WORRY_8', 'MEDIA1', 'MEDIA2', 'MEDIA3', 'MEDIA3_11_TEXT', 'MEDIA4', 'MEDIA4_11_TEXT', 'MEDIA5', 'MEDIA5_8_TEXT', 'MEDIA5_1', 'MEDIA5_1_3_TEXT', 'MEDIA6_1', 'MEDIA6_2', 'MEDIA6_3', 'MEDIA6_4', 'MEDIA6_5', 'MEDIA6_6', 'MEDIA6_7', 'MEDIA7', 'MEDIA8', 'MEDIA8_9_TEXT', 'MEDIA9', 'MEDIA10', 'MEDIA11', 'MEDIA12', 'MEDIA13', 'MEDIA14', 'MEDIA15', 'MEDIA16', 'MEDIA17', 'MH1', 'MH2', 'CHILDCARE1', 'CHILDCARE1_1', 'CHILDCARE1_2', 'CHILDCARE1_3', 'CHILDCARE1_3_6_TEXT', 'CHILDCARE1_3_7_TEXT', 'CHILDCARE2', 'CHILDCARE3', 'CHILDCARE3_1', 'CHILDCARE3_2', 'ELDERLYCARE1', 'IDEOLOGY1', 'IDEOLOGY1_6_TEXT', 'IDEOLOGY2', 'IDEOLOGY2_9_TEXT', 'IDEOLOGY3', 'IDEOLOGY4', 'IDEOLOGY4_5_TEXT', 'IDEOLOGY5', 'IDEOLOGY6', 'IDEOLOGY7', 'IDEOLOGY7_7_TEXT', 'IDEOLOGY8', 'IDEOLOGY8_10_TEXT', 'CULTURE1_1', 'CULTURE1_2', 'CULTURE1_3', 'CULTURE1_4', 'CULTURE1_5', 'CULTURE1_6', 'CULTURE2_1', 'CULTURE2_2', 'CULTURE2_3', 'CULTURE2_4', 'CULTURE2_5', 'CULTURE2_6', 'CULTURE2_7', 'RUMORS1', 'RUMORS1_15_TEXT', 'RUMORS2', 'RUMORS2_9_TEXT', 'DEMOGRAPHIC1', 'DEMOGRAPHIC2', 'DEMOGRAPHIC4', 'DEMOGRAPHIC5', 'DEMOGRAPHIC6', 'DEMOGRAPHIC7', 'DEMOGRAPHIC8', 'DEMOGRAPHIC8_13_TEXT', 'DEMOGRAPHIC9', 'DEMOGRAPHIC10', 'DEMOGRAPHIC10_4_TEXT', 'DEMOGRAPHIC11', 'DEMOGRAPHIC12', 'DEMOGRAPHIC13']\n",
      "161\n",
      "['PROGRESS', 'DURATION', 'HR1_10SCALE', 'ELDERLYCARE1_1', 'DEMOGRAPHIC3']\n"
     ]
    }
   ],
   "source": [
    "strings = raw_df[raw_df.select_dtypes(include=['object']).columns].columns.to_list() \n",
    "bools = raw_df[raw_df.select_dtypes(include=['bool']).columns].columns.to_list() \n",
    "print(bools)\n",
    "print(strings)\n",
    "print(len(strings))\n",
    "intAndFloats = raw_df[raw_df.select_dtypes(include=['int64', 'float64']).columns].columns.to_list() \n",
    "print(intAndFloats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ad4b0",
   "metadata": {},
   "source": [
    "cont: Duration, \n",
    "ratio: demographics3, duration, elderlycare1_1\n",
    "ordinal: HR1_10SCALE\n",
    "nominal: arm, hr1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c7223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check 7 columns that contain int64 or float 64 data, 3 of them are categorical data, move to cat_feature array\n",
    "#convert int columns to string columns because they are categorical data\n",
    "raw_df['HR1_10SCALE'] = raw_df['HR1_10SCALE'].apply(str)\n",
    "raw_df['ARM'] = raw_df['ARM'].apply(str)\n",
    "raw_df['HR1'] = raw_df['HR1'].apply(str)\n",
    "raw_df['FINISHED'] = raw_df['FINISHED'].apply(str)\n",
    "cat_features = strings + bools + ['HR1_10SCALE', 'ARM', 'HR1'] \n",
    "cont_features = [\"PROGRESS\", \"DURATION\", \"DEMOGRAPHIC3\", \"ELDERLYCARE1_1\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b227e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n"
     ]
    }
   ],
   "source": [
    "raw_df.shape \n",
    "totalRows=len(raw_df.index)\n",
    "print(totalRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb06dd",
   "metadata": {},
   "source": [
    "### Summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50dfd85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function for analyzing continuous variables\n",
    "def subString (string):\n",
    "    if isinstance(string, np.integer):\n",
    "        return \"{}\".format(string)\n",
    "    \n",
    "    if type(string)==bool:\n",
    "        return string\n",
    "    \n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    \n",
    "    if len(string) < 10:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:10]  + \"...\"\n",
    "    \n",
    "def cont_summary(feature_name, data): \n",
    "    summary_feature_names = ['Feature', 'Count', '% of Missing', 'Card.', 'Min.', 'Q1', 'Median', \n",
    "                            'Q3', 'Max.', 'Mean', 'Std. Dev.']\n",
    "    # Your answer to Q1 goes here! Make sure to return a DataFrame with the features specified. \n",
    "    newDf = pd.DataFrame(columns=summary_feature_names)\n",
    "    newDf.at[0,'Feature'] = feature_name\n",
    "    newDf.at[0,'Card.'] = data[feature_name].nunique()\n",
    "    newDf.at[0,'Count'] = data[feature_name].count()\n",
    "    newDf.at[0,'% of Missing'] = '{:.2f}%'.format((data[feature_name].isnull().sum()/totalRows)*100)\n",
    "    newDf.at[0,'Min.'] = data[feature_name].min()\n",
    "    newDf.at[0,'Q1'] = data[feature_name].quantile(0.25)\n",
    "    newDf.at[0,'Mean'] = '{:.2f}'.format(data[feature_name].mean())\n",
    "    newDf.at[0,'Median'] = data[feature_name].quantile(0.50)\n",
    "    newDf.at[0,'Q3'] = data[feature_name].quantile(.75)\n",
    "    newDf.at[0,'Max.'] = data[feature_name].max()\n",
    "    newDf.at[0,'Std. Dev.'] = '{:.2f}'.format(data[feature_name].std())\n",
    "    IQR = data[feature_name].quantile(.75) - data[feature_name].quantile(0.25)\n",
    "    Q1 = data[feature_name].quantile(0.25)\n",
    "    Q3 = data[feature_name].quantile(.75)\n",
    "    return newDf\n",
    "\n",
    "#Define Function for analyzing categorical variables\n",
    "def cat_summary(feature_name,data):\n",
    "    summary_feature_names = ['Feature', 'Count', '% of Missing', 'Card.', 'Mode', 'Mode Freq.', 'Mode %', \n",
    "                            '2nd Mode', '2nd Mode Freq.', '2nd Mode Perc']\n",
    "    if feature_name == 'HR1_10SCALE':\n",
    "        data[feature_name] = data[feature_name].astype(\"string\") \n",
    "    newDf = pd.DataFrame(columns=summary_feature_names)\n",
    "    newDf.at[0,'Feature'] = feature_name\n",
    "    newDf.at[0,'Card.'] = data[feature_name].nunique()\n",
    "    newDf.at[0,'Count'] = data[feature_name].count() \n",
    "    newDf.at[0,'% of Missing'] =  '{:.2f}%'.format((data[feature_name].isnull().sum()/totalRows)*100)      \n",
    "    newDf.at[0,'Mode'] = subString(data[feature_name].value_counts().index[0])     \n",
    "    newDf.at[0,'Mode Freq.'] = data[feature_name].value_counts()[0]\n",
    "    newDf.at[0,'Mode %'] = '{:.2f}%'.format((data[feature_name].value_counts()[0]/totalRows)*100)\n",
    "    if (data[feature_name].value_counts()[0]/totalRows)*100 != 100: \n",
    "        newDf.at[0,'2nd Mode'] = subString(data[feature_name].value_counts().index[1])\n",
    "        newDf.at[0,'2nd Mode Freq.'] = data[feature_name].value_counts()[1]\n",
    "        newDf.at[0,'2nd Mode Perc'] = '{:.2f}%'.format((data[feature_name].value_counts()[1]/totalRows)*100)\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2782e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature Count % of Missing Card.  Min.      Q1  Median      Q3  \\\n",
      "0        PROGRESS  1211        0.00%     1   100   100.0   100.0   100.0   \n",
      "1        DURATION  1211        0.00%   983   232   717.5  1168.0  1881.5   \n",
      "2    DEMOGRAPHIC3  1211        0.00%    80  1924  1971.0  1982.0  1990.0   \n",
      "3  ELDERLYCARE1_1   331       72.67%    30   0.0     1.0     1.0     2.0   \n",
      "\n",
      "           Max.         Mean     Std. Dev.  \n",
      "0           100       100.00          0.00  \n",
      "1         45050      1698.93       2354.99  \n",
      "2          2010      1979.17         15.84  \n",
      "3  9867436378.0  29811439.38  542363329.34  \n",
      "                 Feature Count % of Missing Card.           Mode Mode Freq.  \\\n",
      "0              STARTDATE  1211        0.00%   590  6/3/2020 1...         19   \n",
      "1                ENDDATE  1211        0.00%   735  6/3/2020 1...          9   \n",
      "2               FINISHED  1211        0.00%     1           True       1211   \n",
      "3           RECORDEDDATE  1211        0.00%   733  6/3/2020 1...          9   \n",
      "4    DISTRIBUTIONCHANNEL  1211        0.00%     1      anonymous       1211   \n",
      "..                   ...   ...          ...   ...            ...        ...   \n",
      "159        DEMOGRAPHIC12  1211        0.00%     8          White        879   \n",
      "160        DEMOGRAPHIC13  1211        0.00%     5  Middle cla...        502   \n",
      "161          HR1_10SCALE  1211        0.00%    10              1        350   \n",
      "162                  ARM  1211        0.00%     3              0        438   \n",
      "163                  HR1  1211        0.00%     1            nan       1211   \n",
      "\n",
      "      Mode %       2nd Mode 2nd Mode Freq. 2nd Mode Perc  \n",
      "0      1.57%  6/3/2020 1...             13         1.07%  \n",
      "1      0.74%  6/3/2020 1...              7         0.58%  \n",
      "2    100.00%            NaN            NaN           NaN  \n",
      "3      0.74%  6/3/2020 1...              7         0.58%  \n",
      "4    100.00%            NaN            NaN           NaN  \n",
      "..       ...            ...            ...           ...  \n",
      "159   72.58%  Black or A...            158        13.05%  \n",
      "160   41.45%  Lower midd...            260        21.47%  \n",
      "161   28.90%             10            190        15.69%  \n",
      "162   36.17%              1            391        32.29%  \n",
      "163  100.00%            NaN            NaN           NaN  \n",
      "\n",
      "[164 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#analyzing continuous variables\n",
    "summary_feature_names = ['Feature',  'Count', '% of Missing', 'Card.', 'Min.', 'Q1', 'Median','Q3', 'Max.', 'Mean', 'Std. Dev.']\n",
    "quality_cont = pd.DataFrame(columns=summary_feature_names) \n",
    "for i in cont_features:\n",
    "    tmp = cont_summary(i,raw_df)\n",
    "    quality_cont = quality_cont.append(tmp.loc[0], ignore_index=True) \n",
    "print(quality_cont) \n",
    "quality_cont.to_csv(\"cont_summary.csv\")\n",
    "\n",
    "#analyzing categorical variables\n",
    "summary_feature_names = ['Feature',  'Count', '% of Missing', 'Card.', 'Mode', 'Mode Freq.', 'Mode %','2nd Mode', '2nd Mode Freq.', '2nd Mode Perc']\n",
    "quality_cat = pd.DataFrame(columns=summary_feature_names)\n",
    "for i in cat_features:\n",
    "#for i in strings:\n",
    "    #if i!= \"HR1\" or i != 'FINISHED':\n",
    "    tmp = cat_summary(i,raw_df)\n",
    "    quality_cat = quality_cat.append(tmp.loc[0], ignore_index=True)\n",
    "print(quality_cat)\n",
    "\n",
    "quality_cat.to_csv(\"cat_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75039f40",
   "metadata": {},
   "source": [
    "Our main features are these. Of course they are broken into multiple groups for each but this is mainly what we are working with.\n",
    "'STARTDATE', 'ENDDATE', 'PROGRESS', 'DURATION', 'FINISHED', 'RECORDEDDATE', 'DISTRIBUTIONCHANNEL', 'USERLANGUAGE', 'ARM', 'CONSENT', 'COMPREHENSION1_ARM1', 'HR1',  'COVID_HR1', 'STIMULUS1', 'POLICY1', 'PERSONAL1', 'WORRY_1', 'MEDIA1',  'MH1', 'CHILDCARE1','ELDERLYCARE1', 'IDEOLOGY1', 'CULTURE1_1', 'RUMORS1','DEMOGRAPHIC1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e588d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "#Check for unique value counts, if counts are too low, there is no statistics values. \n",
    "single_value_column = []\n",
    "multi_value_card_cat = []\n",
    "for index, row in quality_cat.iterrows():\n",
    "    if quality_cat['Card.'][index] == 1:\n",
    "        single_value_column.append(quality_cat['Feature'][index])\n",
    "    else:\n",
    "        multi_value_card_cat.append(quality_cat['Feature'][index])\n",
    "print(len(single_value_column))\n",
    "print(len(multi_value_card_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b7bf0",
   "metadata": {},
   "source": [
    "### Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfb56954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:  COMPREHENSION1_ARM1 , missing rows:  829 , missing percentage:  68.46%\n",
      "Column:  COMPREHENSION1_ARM2 , missing rows:  829 , missing percentage:  68.46%\n",
      "Column:  COMPREHENSION2_ARM1 , missing rows:  820 , missing percentage:  67.71%\n",
      "Column:  COMPREHENSION2_ARM2 , missing rows:  820 , missing percentage:  67.71%\n",
      "Column:  HR10 , missing rows:  913 , missing percentage:  75.39%\n",
      "Column:  HR11 , missing rows:  2 , missing percentage:  0.17%\n",
      "Column:  HR11_6_TEXT , missing rows:  1177 , missing percentage:  97.19%\n",
      "Column:  HR14_10_TEXT , missing rows:  1203 , missing percentage:  99.34%\n",
      "Column:  COVID_HR1 , missing rows:  485 , missing percentage:  40.05%\n",
      "Column:  COVID_HR2 , missing rows:  404 , missing percentage:  33.36%\n",
      "Column:  COVID_HR2_11_TEXT , missing rows:  1200 , missing percentage:  99.09%\n",
      "Column:  STIMULUS5_8_TEXT , missing rows:  1193 , missing percentage:  98.51%\n",
      "Column:  STIMULUS6_7_TEXT , missing rows:  1166 , missing percentage:  96.28%\n",
      "Column:  POLICY5_13_TEXT , missing rows:  1175 , missing percentage:  97.03%\n",
      "Column:  POLICY6_10_TEXT , missing rows:  1176 , missing percentage:  97.11%\n",
      "Column:  PERSONAL6_13_TEXT , missing rows:  1174 , missing percentage:  96.94%\n",
      "Column:  PERSONAL7_11_TEXT , missing rows:  1197 , missing percentage:  98.84%\n",
      "Column:  MEDIA3_11_TEXT , missing rows:  1188 , missing percentage:  98.10%\n",
      "Column:  MEDIA4_11_TEXT , missing rows:  1175 , missing percentage:  97.03%\n",
      "Column:  MEDIA5_8_TEXT , missing rows:  1138 , missing percentage:  93.97%\n",
      "Column:  MEDIA5_1 , missing rows:  920 , missing percentage:  75.97%\n",
      "Column:  MEDIA5_1_3_TEXT , missing rows:  1192 , missing percentage:  98.43%\n",
      "Column:  MEDIA8_9_TEXT , missing rows:  1139 , missing percentage:  94.05%\n",
      "Column:  CHILDCARE1_1 , missing rows:  635 , missing percentage:  52.44%\n",
      "Column:  CHILDCARE1_2 , missing rows:  635 , missing percentage:  52.44%\n",
      "Column:  CHILDCARE1_3 , missing rows:  635 , missing percentage:  52.44%\n",
      "Column:  CHILDCARE1_3_6_TEXT , missing rows:  1202 , missing percentage:  99.26%\n",
      "Column:  CHILDCARE1_3_7_TEXT , missing rows:  1193 , missing percentage:  98.51%\n",
      "Column:  CHILDCARE3_1 , missing rows:  971 , missing percentage:  80.18%\n",
      "Column:  CHILDCARE3_2 , missing rows:  1007 , missing percentage:  83.15%\n",
      "Column:  ELDERLYCARE1_1 , missing rows:  880 , missing percentage:  72.67%\n",
      "Column:  IDEOLOGY1_6_TEXT , missing rows:  1191 , missing percentage:  98.35%\n",
      "Column:  IDEOLOGY2_9_TEXT , missing rows:  1207 , missing percentage:  99.67%\n",
      "Column:  IDEOLOGY4 , missing rows:  323 , missing percentage:  26.67%\n",
      "Column:  IDEOLOGY4_5_TEXT , missing rows:  1135 , missing percentage:  93.72%\n",
      "Column:  IDEOLOGY7_7_TEXT , missing rows:  1189 , missing percentage:  98.18%\n",
      "Column:  IDEOLOGY8_10_TEXT , missing rows:  1194 , missing percentage:  98.60%\n",
      "Column:  RUMORS1_15_TEXT , missing rows:  1167 , missing percentage:  96.37%\n",
      "Column:  RUMORS2_9_TEXT , missing rows:  1155 , missing percentage:  95.38%\n",
      "Column:  DEMOGRAPHIC1 , missing rows:  2 , missing percentage:  0.17%\n",
      "Column:  DEMOGRAPHIC8_13_TEXT , missing rows:  1086 , missing percentage:  89.68%\n",
      "Column:  DEMOGRAPHIC10_4_TEXT , missing rows:  1207 , missing percentage:  99.67%\n"
     ]
    }
   ],
   "source": [
    "#drop STARTDATE ENDDATE FINISHED\tRECORDEDDATE\tDISTRIBUTIONCHANNEL\n",
    "#For all cat_features, check data lenght to find missing data\n",
    "too_many_missing_value_columns = []\n",
    "totalRows = len(raw_df.index)\n",
    "for (columnName, columnData) in raw_df.iteritems(): \n",
    "    missingCount = columnData.isnull().sum() \n",
    "    percentage = (missingCount/totalRows)*100 \n",
    "    #print(percentage)\n",
    "    if missingCount > 0:        \n",
    "        print(\"Column: \", columnName.strip(), \", missing rows: \", \n",
    "            missingCount, \", missing percentage: \",  '{:.2f}%'.format(percentage)) \n",
    "        if percentage > 50.0:\n",
    "            too_many_missing_value_columns.append(columnName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e536940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 124)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: drop columns with single value or more than 50% missing data\n",
    "for col1 in single_value_column:\n",
    "    if col1 in raw_df.columns:\n",
    "        raw_df.drop(col1, axis = 1, inplace=True)\n",
    "    \n",
    "for col2 in too_many_missing_value_columns:\n",
    "    if col2 in raw_df.columns:\n",
    "        raw_df.drop(col2, axis = 1, inplace=True)\n",
    "\n",
    "#Column numbers are down from 166 to 125\n",
    "raw_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8a44eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 119)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2: drop survey STARTDATE, ENDDATE, PROGRESS, DURATION, FINISHED, RECORDEDDATE \n",
    "columnList = ['STARTDATE', 'ENDDATE', 'PROGRESS', 'DURATION', 'FINISHED', 'RECORDEDDATE']\n",
    "for column in columnList:\n",
    "    if  column in raw_df.columns:\n",
    "        raw_df.drop(column, axis = 1, inplace=True)\n",
    "raw_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "713d539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       24\n",
      "1       23\n",
      "2       27\n",
      "3       51\n",
      "4       37\n",
      "        ..\n",
      "1206    22\n",
      "1207    24\n",
      "1208    20\n",
      "1209    48\n",
      "1210    68\n",
      "Name: age, Length: 1211, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Step 3: cacluate age by giving birth year \n",
    "if 'DEMOGRAPHIC3' in raw_df.columns:\n",
    "    raw_df['age'] = 2020 - raw_df['DEMOGRAPHIC3']\n",
    "    raw_df.drop('DEMOGRAPHIC3', axis = 1, inplace=True)\n",
    "    print(raw_df['age'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a23de89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARM</th>\n",
       "      <th>HR1_10SCALE</th>\n",
       "      <th>HR2_1</th>\n",
       "      <th>HR2_2</th>\n",
       "      <th>HR2_3</th>\n",
       "      <th>HR2_4</th>\n",
       "      <th>HR2_5</th>\n",
       "      <th>HR3</th>\n",
       "      <th>HR4</th>\n",
       "      <th>HR5</th>\n",
       "      <th>...</th>\n",
       "      <th>DEMOGRAPHIC5</th>\n",
       "      <th>DEMOGRAPHIC6</th>\n",
       "      <th>DEMOGRAPHIC7</th>\n",
       "      <th>DEMOGRAPHIC8</th>\n",
       "      <th>DEMOGRAPHIC9</th>\n",
       "      <th>DEMOGRAPHIC10</th>\n",
       "      <th>DEMOGRAPHIC11</th>\n",
       "      <th>DEMOGRAPHIC12</th>\n",
       "      <th>DEMOGRAPHIC13</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>46825</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Roman Catholic (Catholic)</td>\n",
       "      <td>Somewhat important</td>\n",
       "      <td>Male</td>\n",
       "      <td>$20,001-$50,000</td>\n",
       "      <td>White</td>\n",
       "      <td>Lower middle class</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>81650</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Protestant (Baptist, Methodist, Non-denominati...</td>\n",
       "      <td>Somewhat important</td>\n",
       "      <td>Female</td>\n",
       "      <td>$10,001-$20,000</td>\n",
       "      <td>White</td>\n",
       "      <td>Lower middle class</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31027</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Something else (SPECIFY)</td>\n",
       "      <td>Don't know/Refused</td>\n",
       "      <td>Female</td>\n",
       "      <td>&lt; $10,000</td>\n",
       "      <td>White</td>\n",
       "      <td>Lower class/poor</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>60609</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Roman Catholic (Catholic)</td>\n",
       "      <td>Not too important</td>\n",
       "      <td>Female</td>\n",
       "      <td>$20,001-$50,000</td>\n",
       "      <td>White</td>\n",
       "      <td>Lower class/poor</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>39466</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Protestant (Baptist, Methodist, Non-denominati...</td>\n",
       "      <td>Not too important</td>\n",
       "      <td>Female</td>\n",
       "      <td>&lt; $10,000</td>\n",
       "      <td>White</td>\n",
       "      <td>Lower class/poor</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ARM HR1_10SCALE  HR2_1  HR2_2  HR2_3  HR2_4  HR2_5  HR3  HR4  HR5  ...  \\\n",
       "0   1           4      3      1      2      3      5    1    2    3  ...   \n",
       "1   0           4      3      3      4      2      4    3    2    3  ...   \n",
       "2   2          10      0      0      0      0      0    0    0    0  ...   \n",
       "3   2           5      3      3      3      3      3    0    1    2  ...   \n",
       "4   1           5      3      0      1      1      2    0    4    1  ...   \n",
       "\n",
       "   DEMOGRAPHIC5  DEMOGRAPHIC6  DEMOGRAPHIC7  \\\n",
       "0         46825           Yes            No   \n",
       "1         81650           Yes            No   \n",
       "2         31027           Yes            No   \n",
       "3         60609           Yes            No   \n",
       "4         39466           Yes            No   \n",
       "\n",
       "                                        DEMOGRAPHIC8        DEMOGRAPHIC9  \\\n",
       "0                          Roman Catholic (Catholic)  Somewhat important   \n",
       "1  Protestant (Baptist, Methodist, Non-denominati...  Somewhat important   \n",
       "2                           Something else (SPECIFY)  Don't know/Refused   \n",
       "3                          Roman Catholic (Catholic)   Not too important   \n",
       "4  Protestant (Baptist, Methodist, Non-denominati...   Not too important   \n",
       "\n",
       "   DEMOGRAPHIC10    DEMOGRAPHIC11  DEMOGRAPHIC12       DEMOGRAPHIC13  age  \n",
       "0           Male  $20,001-$50,000          White  Lower middle class   24  \n",
       "1         Female  $10,001-$20,000          White  Lower middle class   23  \n",
       "2         Female        < $10,000          White    Lower class/poor   27  \n",
       "3         Female  $20,001-$50,000          White    Lower class/poor   51  \n",
       "4         Female        < $10,000          White    Lower class/poor   37  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4: Identify the Ordinal Data \n",
    "cleaned_df=raw_df\n",
    "neg_pos_dic1 = {\"Don't Know\":0, \"Very Negative\": 1, \"Somewhat Negative\":2, \"Neutral\":3, \"Somewhat Positive\":4, \n",
    "               \"Very Positive\":5}\n",
    " \n",
    "cleaned_df[\"HR2_1\"].replace(neg_pos_dic1, inplace=True) \n",
    "cleaned_df[\"HR2_2\"].replace(neg_pos_dic1, inplace=True) \n",
    "cleaned_df[\"HR2_3\"].replace(neg_pos_dic1, inplace=True) \n",
    "cleaned_df[\"HR2_4\"].replace(neg_pos_dic1, inplace=True)  \n",
    "cleaned_df[\"HR2_5\"].replace(neg_pos_dic1, inplace=True)  \n",
    "#cleaned_df.to_csv(\"step2_cleaned_df.csv\")\n",
    "\n",
    "dissat_satis_dic = {\"No opinion\": 0, \"Very dissatisfied\": 1, \"Somewhat dissatisfied\": 2, \n",
    "                    \"Somewhat satisfied\": 3, \"Very satisfied\": 4}\n",
    "cleaned_df[\"HR3\"].replace(dissat_satis_dic, inplace=True)\n",
    "cleaned_df[\"HR5\"].replace(dissat_satis_dic, inplace=True)\n",
    "\n",
    "poor_excellent_dic = {\"No opinion\": 0,\"Poor\": 1, \"Fair\":2, \"Good\": 3, \"Excellent\": 4}\n",
    "cleaned_df[\"HR4\"].replace(poor_excellent_dic, inplace=True)\n",
    "cleaned_df[\"HR6\"].replace(poor_excellent_dic, inplace=True)\n",
    "\n",
    "opp_favor_dic1 = {\"Unsure\": 0, \"Strongly Oppose\": 1, \"Oppose\": 2, \"Favor\": 3, \"Strongly Favor\": 4}\n",
    "cleaned_df[\"HR7_1\"].replace(opp_favor_dic1, inplace=True)\n",
    "cleaned_df[\"HR7_2\"].replace(opp_favor_dic1, inplace=True)\n",
    "cleaned_df[\"HR7_3\"].replace(opp_favor_dic1, inplace=True)\n",
    "cleaned_df[\"HR7_4\"].replace(opp_favor_dic1, inplace=True)\n",
    "\n",
    "opp_favor_dic2 = {\"Don't know\": 0, \"Strongly oppose\": 1, \"Somewhat oppose\": 1, \"Somewhat favor\": 2, \"Strongly favor\": 3}\n",
    "cleaned_df[\"HR7_1\"].replace(opp_favor_dic2, inplace=True)\n",
    "cleaned_df[\"HR7_2\"].replace(opp_favor_dic2, inplace=True)\n",
    "cleaned_df[\"HR7_3\"].replace(opp_favor_dic2, inplace=True)\n",
    "cleaned_df[\"HR7_4\"].replace(opp_favor_dic2, inplace=True)\n",
    "cleaned_df[\"HR8\"].replace(opp_favor_dic2, inplace=True)\n",
    "\n",
    "neg_pos_dic2 = {\"No Opinion\":0, \"Mostly Negative\": 1, \"Mostly Positive\":2}\n",
    "cleaned_df[\"HR12_1\"].replace(neg_pos_dic2, inplace=True) \n",
    "cleaned_df[\"HR12_2\"].replace(neg_pos_dic2, inplace=True) \n",
    "cleaned_df[\"HR12_3\"].replace(neg_pos_dic2, inplace=True) \n",
    "cleaned_df[\"HR12_4\"].replace(neg_pos_dic2, inplace=True)  \n",
    "cleaned_df[\"HR12_5\"].replace(neg_pos_dic2, inplace=True)\n",
    "\n",
    "adequate_dic1 = {\"Don't know/unsure\": 0, \"Not adequate at all\": 1, \"Adequate\": 2, \"More than adequate\": 3}\n",
    "cleaned_df[\"STIMULUS3\"].replace(adequate_dic1, inplace=True) \n",
    "cleaned_df[\"STIMULUS4\"].replace(adequate_dic1, inplace=True)\n",
    "\n",
    "likely_dic1 = {\"Don't know/unsure\": 0, \"Highly unlikely\": 1, \"Unlikely\": 2, \"Likely\": 3, \"Highly likely\": 4}\n",
    "cleaned_df[\"PERSONAL3\"].replace(likely_dic1, inplace=True)\n",
    "\n",
    "disagree_agree_dic1 = {\"Don't know/unsure\": 0, \"Strongly disagree\": 1, \"Disagree\": 2, \"Agree\": 3, \"Strongly agree\": 4}\n",
    "cleaned_df[\"PERSONAL4\"].replace(disagree_agree_dic1, inplace=True)\n",
    "\n",
    "worry_dic1 = {\"Not worried at all\": 0, \"Not very worried\": 1, \"Somewhat Worried\": 2, \"Very Worried\": 3}\n",
    "cleaned_df[\"WORRY_1\"].replace(worry_dic1, inplace=True)\n",
    "cleaned_df[\"WORRY_2\"].replace(worry_dic1, inplace=True)\n",
    "cleaned_df[\"WORRY_3\"].replace(worry_dic1, inplace=True)\n",
    "cleaned_df[\"WORRY_4\"].replace(worry_dic1, inplace=True)\n",
    "cleaned_df[\"WORRY_5\"].replace(worry_dic1, inplace=True)\n",
    "cleaned_df[\"WORRY_6\"].replace(worry_dic1, inplace=True)\n",
    "cleaned_df[\"WORRY_7\"].replace(worry_dic1, inplace=True)\n",
    "cleaned_df[\"WORRY_8\"].replace(worry_dic1, inplace=True)\n",
    "\n",
    "often_dic1 = {\"Very Little (less than weekly)\": 0, \"Somewhat (weekly)\": 1, \"A lot (daily)\": 2, \"Frequent (multiple times a day)\": 3}\n",
    "cleaned_df[\"MEDIA1\"].replace(often_dic1, inplace=True)\n",
    "\n",
    "confidence_dic1 = {\"Unsure\": 0, \"No confidence at all\": 1, \"Very little confidence\": 2, \"Some confidence\": 3,  \"A great deal of confidence\": 4,\"Complete confidence\": 5}\n",
    "cleaned_df[\"MEDIA6_1\"].replace(confidence_dic1, inplace=True)\n",
    "cleaned_df[\"MEDIA6_2\"].replace(confidence_dic1, inplace=True)\n",
    "cleaned_df[\"MEDIA6_3\"].replace(confidence_dic1, inplace=True)\n",
    "cleaned_df[\"MEDIA6_4\"].replace(confidence_dic1, inplace=True)\n",
    "cleaned_df[\"MEDIA6_5\"].replace(confidence_dic1, inplace=True)\n",
    "cleaned_df[\"MEDIA6_6\"].replace(confidence_dic1, inplace=True)\n",
    "cleaned_df[\"MEDIA6_7\"].replace(confidence_dic1, inplace=True)\n",
    " \n",
    "bad_good_dic1 = {\"Very bad\": 0, \"Bad\": 1, \"Fair\": 2, \"Good\": 3, \"Very good\": 4}\n",
    "cleaned_df[\"MEDIA7\"].replace(bad_good_dic1, inplace=True)\n",
    "\n",
    "disagree_agree_dic2 = {\"Don't know\": 0, \"Strongly disagree\": 1, \"Somewhat disagree\": 2, \"Somewhat Agree\": 3, \"Strongly agree\": 4}\n",
    "cleaned_df[\"MEDIA10\"].replace(disagree_agree_dic2, inplace=True)\n",
    "\n",
    "smoker_dic1 = {\"Never smoked\": 0, \"Former smoker, occasional\": 1, \"Former smoker, frequent\": 2, \n",
    "               \"Current Smoker, occasional\": 3, \"Current Smoker, frequent\": 4}\n",
    "cleaned_df[\"MEDIA11\"].replace(smoker_dic1, inplace=True)\n",
    " \n",
    "smoker_dic2 = {\"Not at all\": 0, \"Some days\": 1, \"Every day\": 2}\n",
    "cleaned_df[\"MEDIA12\"].replace(smoker_dic2, inplace=True) \n",
    "\n",
    "culture1_dic1 = {\"Agree Strongly\":5, \"Agree\":4, \"Neither agree nor disagree\":3, \"Disagree\":2, \"Disagree Strongly\":1}\n",
    "cleaned_df[\"CULTURE1_1\"].replace(culture1_dic1, inplace=True)\n",
    "cleaned_df[\"CULTURE1_2\"].replace(culture1_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE1_3\"].replace(culture1_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE1_4\"].replace(culture1_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE1_5\"].replace(culture1_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE1_6\"].replace(culture1_dic1, inplace=True)\n",
    "\n",
    "culture2_dic1 = {\"Agree Strongly\":5, \"Agree\":4, \"Neither agree nor disagree\":3, \"Disagree\":2, \"Disagree Strongly\":1}\n",
    "cleaned_df[\"CULTURE2_1\"].replace(culture2_dic1, inplace=True)\n",
    "cleaned_df[\"CULTURE2_2\"].replace(culture2_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE2_3\"].replace(culture2_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE2_4\"].replace(culture2_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE2_5\"].replace(culture2_dic1, inplace=True) \n",
    "cleaned_df[\"CULTURE2_6\"].replace(culture2_dic1, inplace=True)\n",
    "cleaned_df[\"CULTURE2_7\"].replace(culture2_dic1, inplace=True)\n",
    "\n",
    "cleaned_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5a98c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: For long long long text data, replace with survey answer index or A, B, C, D, E\n",
    "#because the text is so long, dictionary contains key works to find in the text\n",
    "#Clean up HR11\n",
    "cleaned_df.loc[cleaned_df['HR11'].astype(str).str.startswith('Incrementally building on'),  'HR11'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['HR11'].astype(str).str.startswith('Creating a universal'),  'HR11'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['HR11'].astype(str).str.startswith('Reversing the Affordable'),  'HR11'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['HR11'].astype(str).str.startswith('Something else'),  'HR11'] = \"D\"\n",
    "\n",
    "#Clean up HR13\n",
    "cleaned_df.loc[cleaned_df['HR13'].astype(str).str.startswith('Yes, I lost my health insurance due to'),  'HR13'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['HR13'].astype(str).str.startswith('Yes, I lost my health insurance for'),  'HR13'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['HR13'].astype(str).str.startswith('Yes, somebody close to me lost their health insurance due'),  \n",
    "               'HR13'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['HR13'].astype(str).str.startswith('Yes, somebody close to me lost their health insurance for'), \n",
    "               'HR13'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['HR13'].astype(str).str.startswith('No'),  'HR13'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['HR13'].astype(str).str.startswith('Unsure'),  'HR13'] = \"F\"\n",
    "\n",
    "#Clean up HR14\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('through an employer') >= 0,  'HR14'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('with a subsidy') >= 0,  'HR14'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('with no subsidy') >= 0, 'HR14'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('None, uninsured') >= 0, 'HR14'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('Medicaid') >= 0, 'HR14'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('Medicare') >= 0, 'HR14'] = \"F\"\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('Veteran') >= 0, 'HR14'] = \"G\"\n",
    "cleaned_df.loc[cleaned_df['HR14'].astype(str).str.find('Other,') >= 0, 'HR14'] = \"H\"\n",
    "\n",
    "#clean up COVID_HR1 a little bit\n",
    "cleaned_df.loc[cleaned_df['COVID_HR1'].astype(str).str.startswith('Some-'),  'COVID_HR1'] = \"Some\"\n",
    "\n",
    "#clean up COVID_HR2\n",
    "cleaned_df.loc[cleaned_df['COVID_HR2'].astype(str).str.find('The government') >= 0,  'COVID_HR2'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['COVID_HR2'].astype(str).str.find('Health insurance companies') >= 0,  'COVID_HR2'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['COVID_HR2'].astype(str).str.find('A combination of both') >= 0, 'COVID_HR2'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['COVID_HR2'].astype(str).str.find('Neither,') >= 0, 'COVID_HR2'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['COVID_HR2'].astype(str).str.find('Something else') >= 0, 'COVID_HR2'] = \"E\"\n",
    "\n",
    "#clean up COVID_HR3\n",
    "cleaned_df.loc[cleaned_df['COVID_HR3'].astype(str).str.find('more favorable') >= 0,  'COVID_HR3'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['COVID_HR3'].astype(str).str.find('less favorable') >= 0,  'COVID_HR3'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['COVID_HR3'].astype(str).str.find('not affected') >= 0, 'COVID_HR3'] = \"C\" \n",
    "\n",
    "#clean up STIMULUS2\n",
    "cleaned_df.loc[cleaned_df['STIMULUS2'].astype(str).str.find('somebody close to me') >= 0,  'STIMULUS2'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS2'].astype(str).str.find('immediate family') >= 0,  'STIMULUS2'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS2'].astype(str).str.find('No') >= 0, 'STIMULUS2'] = \"C\"\n",
    " \n",
    "#clean up STIMULUS5\n",
    "cleaned_df.loc[cleaned_df['STIMULUS5'].astype(str).str.find('check from the government') >= 0,  'STIMULUS5'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS5'].astype(str).str.find('small business loan') >= 0,  'STIMULUS5'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS5'].astype(str).str.find('employment benefits') >= 0, 'STIMULUS5'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS5'].astype(str).str.find('No') >= 0,  'STIMULUS5'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS5'].astype(str).str.find('Unsure') >= 0, 'STIMULUS5'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS5'].astype(str).str.find('Other') >= 0, 'STIMULUS5'] = \"F\"\n",
    "\n",
    "#clean up STIMULUS6\n",
    "cleaned_df.loc[cleaned_df['STIMULUS6'].astype(str).str.find('market run its course') >= 0,  'STIMULUS6'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS6'].astype(str).str.find('direct payments') >= 0,  'STIMULUS6'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS6'].astype(str).str.find('stimulus for small') >= 0, 'STIMULUS6'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS6'].astype(str).str.find('Guarantee the salary') >= 0,  'STIMULUS6'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['STIMULUS6'].astype(str).str.find('Other,') >= 0, 'STIMULUS6'] = \"E\"\n",
    "\n",
    "#clean up policy1\n",
    "cleaned_df.loc[cleaned_df['POLICY1'].astype(str).str.find('Not at all') >= 0,  'POLICY1'] = 0\n",
    "cleaned_df.loc[cleaned_df['POLICY1'].astype(str).str.find('A little') >= 0,  'POLICY1'] = 1\n",
    "cleaned_df.loc[cleaned_df['POLICY1'].astype(str).str.find('A lot') >= 0, 'POLICY1'] = 2 \n",
    " \n",
    "#clean up policy2\n",
    "cleaned_df.loc[cleaned_df['POLICY2'].astype(str).str.find('Not at all') >= 0,  'POLICY2'] = 0\n",
    "cleaned_df.loc[cleaned_df['POLICY2'].astype(str).str.find('Somewhat concerned') >= 0,  'POLICY2'] = 1\n",
    "cleaned_df.loc[cleaned_df['POLICY2'].astype(str).str.find('Very concerned') >= 0, 'POLICY2'] = 2\n",
    "\n",
    "#clean up policy3\n",
    "cleaned_df.loc[cleaned_df['POLICY3'].astype(str).str.find('overreaction') >= 0,  'POLICY3'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['POLICY3'].astype(str).str.find('serious illness') >= 0,  'POLICY3'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['POLICY3'].astype(str).str.find('large threat') >= 0, 'POLICY3'] = \"C\" \n",
    "\n",
    "#clean up policy4\n",
    "cleaned_df.loc[cleaned_df['POLICY4'].astype(str).str.find('economy is a bigger threat') >= 0,  'POLICY4'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['POLICY4'].astype(str).str.find('People should not put economic') >= 0,  'POLICY4'] = \"B\"\n",
    " \n",
    "#clean up Personal2\n",
    "cleaned_df.loc[cleaned_df['PERSONAL2'].astype(str).str.find('None of the time') >= 0,  'PERSONAL2'] = 0\n",
    "cleaned_df.loc[cleaned_df['PERSONAL2'].astype(str).str.find('Some of the time') >= 0,  'PERSONAL2'] = 1\n",
    "cleaned_df.loc[cleaned_df['PERSONAL2'].astype(str).str.find('Most of the time') >= 0, 'PERSONAL2'] = 2 \n",
    "cleaned_df.loc[cleaned_df['PERSONAL2'].astype(str).str.find('All of the time') >= 0, 'PERSONAL2'] = 3 \n",
    "\n",
    "#clean up MEDIA2\n",
    "cleaned_df.loc[cleaned_df['MEDIA2'].astype(str).str.find('began in China') >= 0,  'MEDIA2'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA2'].astype(str).str.find('began in Italy') >= 0,  'MEDIA2'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA2'].astype(str).str.find('began in the US') >= 0, 'MEDIA2'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA2'].astype(str).str.find('not paying close attention') >= 0,  'MEDIA2'] = \"D\"\n",
    "\n",
    "#clean up MEDIA5\n",
    "cleaned_df.loc[cleaned_df['MEDIA5'].astype(str).str.find('Fox News') >= 0,  'MEDIA5'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA5'].astype(str).str.find('MSNBC') >= 0,  'MEDIA5'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA5'].astype(str).str.find('CNN') >= 0, 'MEDIA5'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA5'].astype(str).str.find('Evening news or morning shows') >= 0,  'MEDIA5'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA5'].astype(str).str.find('Alternative news media outlets') >= 0, 'MEDIA5'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA5'].astype(str).str.find('Other') >= 0,  'MEDIA5'] = \"F\"\n",
    "\n",
    "#clean up Personal2\n",
    "cleaned_df.loc[cleaned_df['MEDIA10'].astype(str).str.find(\"Don't know\") >= 0,  'MEDIA10'] = 0\n",
    "cleaned_df.loc[cleaned_df['MEDIA10'].astype(str).str.find(\"Strongly Disagree\") >= 0,  'MEDIA10'] = 1\n",
    "cleaned_df.loc[cleaned_df['MEDIA10'].astype(str).str.find(\"Somewhat Disagree\") >= 0, 'MEDIA10'] = 2 \n",
    "cleaned_df.loc[cleaned_df['MEDIA10'].astype(str).str.find(\"Somewhat Agree\") >= 0, 'MEDIA10'] = 3 \n",
    "cleaned_df.loc[cleaned_df['MEDIA10'].astype(str).str.find(\"Strongly Agree\") >= 0,  'MEDIA10'] = 4\n",
    " \n",
    "#clean up Media13\n",
    "cleaned_df.loc[cleaned_df['MEDIA13'].astype(str).str.find('Yes') >= 0,  'MEDIA13'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA13'].astype(str).str.find('No') >= 0,  'MEDIA13'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA13'].astype(str).str.find(\"Don't know\") >= 0, 'MEDIA13'] = \"C\"\n",
    "\n",
    "#clean up Media14\n",
    "cleaned_df.loc[cleaned_df['MEDIA14'].astype(str).str.find('Yes') >= 0,  'MEDIA15'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA14'].astype(str).str.find('No') >= 0,  'MEDIA15'] = \"B\"\n",
    "\n",
    "#clean up MEDIA15\n",
    "cleaned_df.loc[cleaned_df['MEDIA15'].astype(str).str.find('test was positive') >= 0,  'MEDIA15'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA15'].astype(str).str.find('test was negative') >= 0,  'MEDIA15'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA15'].astype(str).str.find('could not get a test') >= 0, 'MEDIA15'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA15'].astype(str).str.find('not tried to be tested') >= 0,  'MEDIA15'] = \"D\" \n",
    "\n",
    "#clean up Media17\n",
    "cleaned_df.loc[cleaned_df['MEDIA17'].astype(str).str.find('Yes') >= 0,  'MEDIA17'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA17'].astype(str).str.find('No') >= 0,  'MEDIA17'] = \"B\"\n",
    "\n",
    "#clean up Media9 \n",
    "cleaned_df.loc[cleaned_df['MEDIA9'].astype(str).str.find('Yes') >= 0,  'MEDIA9'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA9'].astype(str).str.find('No') >= 0,  'MEDIA9'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['MEDIA9'].astype(str).str.find(\"Don't remember\") >= 0, 'MEDIA9'] = \"C\"\n",
    "    \n",
    "#clean up MH1\n",
    "cleaned_df.loc[cleaned_df['MH1'].astype(str).str.find('Not at all') >= 0,  'MH1'] = 0\n",
    "cleaned_df.loc[cleaned_df['MH1'].astype(str).str.find('Several days') >= 0,  'MH1'] = 1\n",
    "cleaned_df.loc[cleaned_df['MH1'].astype(str).str.find('More than half the days') >= 0, 'MH1'] = 2\n",
    "cleaned_df.loc[cleaned_df['MH1'].astype(str).str.find('Nearly every day') >= 0, 'MH1'] = 3\n",
    "\n",
    "#clean up MH2\n",
    "cleaned_df.loc[cleaned_df['MH2'].astype(str).str.find('Not at all') >= 0,  'MH2'] = 0\n",
    "cleaned_df.loc[cleaned_df['MH2'].astype(str).str.find('Several days') >= 0,  'MH2'] = 1\n",
    "cleaned_df.loc[cleaned_df['MH2'].astype(str).str.find('More than half the days') >= 0, 'MH2'] = 2\n",
    "cleaned_df.loc[cleaned_df['MH2'].astype(str).str.find('Nearly every day') >= 0, 'MH2'] = 3\n",
    "\n",
    "#clean up CHILDCARE2\n",
    "cleaned_df.loc[cleaned_df['CHILDCARE2'].astype(str).str.find('Yes') >= 0,  'CHILDCARE2'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['CHILDCARE2'].astype(str).str.find('No') >= 0,  'CHILDCARE2'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['CHILDCARE2'].astype(str).str.find(\"Don't Know\") >= 0, 'CHILDCARE2'] = \"C\" \n",
    "\n",
    "#clean up IDEOLOGY1\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY1'].astype(str).str.find('Democrat') >= 0,  'IDEOLOGY1'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY1'].astype(str).str.find('Republican') >= 0,  'IDEOLOGY1'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY1'].astype(str).str.find('Independent') >= 0, 'IDEOLOGY1'] = \"C\" \n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY1'].astype(str).str.find('Other,') >= 0, 'IDEOLOGY1'] = \"D\"\n",
    "\n",
    "#clean up IDEOLOGY2\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Democrat') >= 0,  'IDEOLOGY2'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Republican') >= 0,  'IDEOLOGY2'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Tea Party') >= 0, 'IDEOLOGY2'] = \"C\" \n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Libertarian Party') >= 0, 'IDEOLOGY2'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Working Families') >= 0, 'IDEOLOGY2'] = \"E\" \n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Green Party') >= 0, 'IDEOLOGY2'] = \"F\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Socialist Party of America') >= 0, 'IDEOLOGY2'] = \"G\" \n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('None') >= 0, 'IDEOLOGY2'] = \"H\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY2'].astype(str).str.find('Other,') >= 0, 'IDEOLOGY2'] = \"I\"\n",
    "\n",
    "#clean up IDEOLOGY3\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY3'].astype(str).str.find('Yes') >= 0,  'IDEOLOGY3'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY3'].astype(str).str.find('No, not planning') >= 0,  'IDEOLOGY3'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY3'].astype(str).str.find('Primary was') >= 0, 'IDEOLOGY3'] = \"C\" \n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY3'].astype(str).str.find(\"Don't know\") >= 0, 'IDEOLOGY3'] = \"D\"\n",
    "\n",
    "#clean up IDEOLOGY4\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY4'].astype(str).str.find('Bernie Sanders') >= 0,  'IDEOLOGY4'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY4'].astype(str).str.find('Joe Biden') >= 0,  'IDEOLOGY4'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY4'].astype(str).str.find('Someone else') >= 0, 'IDEOLOGY4'] = \"C\" \n",
    "\n",
    "#clean up IDEOLOGY5\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY5'].astype(str).str.find('Far Right') >= 0,  'IDEOLOGY5'] = 2\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY5'].astype(str).str.find('Center Right') >= 0,  'IDEOLOGY5'] = 1\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY5'].astype(str).str.find('Neither Right') >= 0, 'IDEOLOGY5'] = 0\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY5'].astype(str).str.find('Center Left') >= 0,  'IDEOLOGY5'] = -1\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY5'].astype(str).str.find('Far Left') >= 0, 'IDEOLOGY5'] = -2\n",
    "\n",
    "#clean up IDEOLOGY6\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY6'].astype(str).str.find('Deeply') >= 0,  'IDEOLOGY6'] = 4\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY6'].astype(str).str.find('Somewhat political') >= 0,  'IDEOLOGY6'] = 3\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY6'].astype(str).str.find('Neither') >= 0, 'IDEOLOGY6'] = 2\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY6'].astype(str).str.find('Somewhat unpolitical') >= 0,  'IDEOLOGY6'] = 1\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY6'].astype(str).str.find('Not political') >= 0, 'IDEOLOGY6'] = 0\n",
    "\n",
    "#clean up IDEOLOGY7\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY7'].astype(str).str.find('Did not vote') >= 0,  'IDEOLOGY7'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY7'].astype(str).str.find('Was not eligible to') >= 0,  'IDEOLOGY7'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY7'].astype(str).str.find('Donald Trump') >= 0, 'IDEOLOGY7'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY7'].astype(str).str.find('Hillary Clinton') >= 0,  'IDEOLOGY7'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY7'].astype(str).str.find('Someone else') >= 0, 'IDEOLOGY7'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY7'].astype(str).str.find(\"Don't remember\") >= 0, 'IDEOLOGY7'] = \"F\"\n",
    "\n",
    "#clean up IDEOLOGY8\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY8'].astype(str).str.find('Not planning to vote') >= 0,  'IDEOLOGY8'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY8'].astype(str).str.find('Not eligible to vote') >= 0,  'IDEOLOGY8'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY8'].astype(str).str.find('Donald Trump') >= 0, 'IDEOLOGY8'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY8'].astype(str).str.find('Joe Biden') >= 0,  'IDEOLOGY8'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY8'].astype(str).str.find('Have not yet') >= 0, 'IDEOLOGY8'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['IDEOLOGY8'].astype(str).str.find(\"Write in other\") >= 0, 'IDEOLOGY8'] = \"F\"\n",
    "\n",
    "#clean up RUMORS2\n",
    "cleaned_df.loc[cleaned_df['RUMORS2'].astype(str).str.find('accidently') >= 0,  'RUMORS2'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['RUMORS2'].astype(str).str.find('purposefully') >= 0,  'RUMORS2'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['RUMORS2'].astype(str).str.find('The US army') >= 0, 'RUMORS2'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['RUMORS2'].astype(str).str.find('bat to humans') >= 0,  'RUMORS2'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['RUMORS2'].astype(str).str.find('bats to another animal') >= 0, 'RUMORS2'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['RUMORS2'].astype(str).str.find('It was introduced in a') >= 0, 'RUMORS2'] = \"F\"\n",
    "cleaned_df.loc[cleaned_df['RUMORS2'].astype(str).str.find('Other,') >= 0, 'RUMORS2'] = \"G\"\n",
    "\n",
    "#clean up DEMOGRAPHIC8\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Protestant') >= 0,  'DEMOGRAPHIC8'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Roman') >= 0,  'DEMOGRAPHIC8'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Mormon') >= 0, 'DEMOGRAPHIC8'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Orthodox') >= 0,  'DEMOGRAPHIC8'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Jewish') >= 0, 'DEMOGRAPHIC8'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Muslim') >= 0, 'DEMOGRAPHIC8'] = \"F\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Buddhist') >= 0, 'DEMOGRAPHIC8'] = \"G\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Hindu') >= 0, 'DEMOGRAPHIC8'] = \"H\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Atheist') >= 0, 'DEMOGRAPHIC8'] = \"I\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Agnostic') >= 0, 'DEMOGRAPHIC8'] = \"J\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Agnostic') >= 0, 'DEMOGRAPHIC8'] = \"K\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC8'].astype(str).str.find('Something') >= 0, 'DEMOGRAPHIC8'] = \"L\"\n",
    "\n",
    "#clean up DEMOGRAPHIC9\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC9'].astype(str).str.find('Very important') >= 0,  'DEMOGRAPHIC9'] = 4\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC9'].astype(str).str.find('Somewhat important') >= 0,  'DEMOGRAPHIC9'] = 3\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC9'].astype(str).str.find('Not too important') >= 0, 'DEMOGRAPHIC9'] = 2\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC9'].astype(str).str.find('Not at all important') >= 0,  'DEMOGRAPHIC9'] = 1\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC9'].astype(str).str.find(\"Don't know/Refused\") >= 0, 'DEMOGRAPHIC9'] = 0\n",
    "\n",
    "#clean up DEMOGRAPHIC11\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('< $10,000') >= 0,  'DEMOGRAPHIC11'] = 0\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('$10,001-') >= 0,  'DEMOGRAPHIC11'] = 1\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('$20,001-') >= 0, 'DEMOGRAPHIC11'] = 2\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('$50,001-') >= 0,  'DEMOGRAPHIC11'] = 3\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('$75,001-') >= 0, 'DEMOGRAPHIC11'] = 4\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('$150,001-') >= 0, 'DEMOGRAPHIC11'] = 5\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('>$201,000-') >= 0, 'DEMOGRAPHIC11'] = 6\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC11'].astype(str).str.find('$250,001+') >= 0, 'DEMOGRAPHIC11'] = 7\n",
    "\n",
    "#clean up DEMOGRAPHIC12\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('White') >= 0,  'DEMOGRAPHIC12'] = \"A\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('Hispanic') >= 0,  'DEMOGRAPHIC12'] = \"B\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('Black') >= 0, 'DEMOGRAPHIC12'] = \"C\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('Asian') >= 0,  'DEMOGRAPHIC12'] = \"D\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('American') >= 0, 'DEMOGRAPHIC12'] = \"E\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('Middle Eastern') >= 0, 'DEMOGRAPHIC12'] = \"F\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('Native Hawaiian') >= 0, 'DEMOGRAPHIC12'] = \"G\"\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC12'].astype(str).str.find('Some other Race') >= 0, 'DEMOGRAPHIC12'] = \"H\"\n",
    "\n",
    "#clean up DEMOGRAPHIC13\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC13'].astype(str).str.find('Lower') >= 0,  'DEMOGRAPHIC13'] = 0\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC13'].astype(str).str.find('Middle') >= 0,  'DEMOGRAPHIC13'] = 1\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC13'].astype(str).str.find('Upper middle') >= 0, 'DEMOGRAPHIC13'] = 2\n",
    "cleaned_df.loc[cleaned_df['DEMOGRAPHIC13'].astype(str).str.find('Upper class') >= 0,  'DEMOGRAPHIC13'] = 3\n",
    "\n",
    "\n",
    "#def replaceLongText (string_data, long_text_dic):\n",
    "#    for key, value in long_text_dic.items(): \n",
    "#        if string_data.astype(str).str.find(key) >= 0:\n",
    "#            return value\n",
    "            \n",
    "#HR11_dic = {\"Incrementally building on\":\"A\", \"Creating a universal\": \"B\", \n",
    "#            \"Reversing the Affordable\": \"C\", \"Something else,\":\"D\"}\n",
    "\n",
    "#Not working \n",
    "#cleaned_df['HR11'] = (cleaned_df.apply(lambda x: replaceLongText(x[x['HR11']], HR11_dic), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3cbf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For multiple answer questions, need create extra columns for every question\n",
    "#If you run the second time, since column '' has been dropped, you will see the error message, add checking column \n",
    "if 'STIMULUS1' in cleaned_df.columns:\n",
    "    cleaned_df['STIMULUS1_WORK_FROM_HOME'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Work from home') >= 0 else 0) \n",
    "    cleaned_df['STIMULUS1_LOST_JOB'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Lost job') >= 0 else 0) \n",
    "    cleaned_df['STIMULUS1_FURLOUGHED'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Furloughed') >= 0 else 0) \n",
    "    cleaned_df['STIMULUS1_REDUCED_HOURS'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Reduced hours') >= 0 else 0) \n",
    "    cleaned_df['STIMULUS1_PAY_CUT'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Pay cut') >= 0 else 0) \n",
    "    cleaned_df['STIMULUS1_WORK_MORE_HOUR'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Work more hours') >= 0 else 0) \n",
    "    cleaned_df['STIMULUS1_INCREASED_PAY'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Increased pay') >= 0 else 0) \n",
    "    cleaned_df['STIMULUS1_Nothing changed'] = cleaned_df['STIMULUS1'].apply(lambda x: 1 if x.find('Nothing changed') >= 0 else 0) \n",
    "    cleaned_df.drop('STIMULUS1', axis = 1, inplace=True) \n",
    "\n",
    "if 'POLICY5' in cleaned_df.columns:\n",
    "    cleaned_df['POLICY5_EVERYONE_SD'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('Require everyone') >= 0 else 0) \n",
    "    cleaned_df['POLICY5_AFFECTED_AREA_SD'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('heavily affected areas') >= 0 else 0) \n",
    "    cleaned_df['POLICY5_VOLUNTARY_SD'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('voluntary social distancing') >= 0 else 0) \n",
    "    cleaned_df['POLICY5_OLD_PEOPLE_SD'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('older people age 66') >= 0 else 0) \n",
    "    cleaned_df['POLICY5_CLOSE_SCHOOL'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('Close schools') >= 0 else 0) \n",
    "    cleaned_df['POLICY5_CLOSE_UNIVERSITY'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('Close universities') >= 0 else 0) \n",
    "    cleaned_df['POLICY5_CANCEL_LARGE_EVENT'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('Cancel large events') >= 0 else 0) \n",
    "    cleaned_df['POLICY5_OTHER'] = cleaned_df['POLICY5'].apply(lambda x: 1 if x.find('OTHER') >= 0 else 0) \n",
    "    cleaned_df.drop('POLICY5', axis = 1, inplace=True) \n",
    "    \n",
    "if 'POLICY6' in cleaned_df.columns:\n",
    "    cleaned_df['POLICY6_EVERYONE_SD'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('practice social distancing') >= 0 else 0) \n",
    "    cleaned_df['POLICY6_AFFECTED_AREA_SD'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('heavily affected areas') >= 0 else 0) \n",
    "    cleaned_df['POLICY6_VOLUNTARY_SD'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('voluntary social distancing') >= 0 else 0) \n",
    "    cleaned_df['POLICY6_OLD_PEOPLE_SD'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('older people age 66') >= 0 else 0) \n",
    "    cleaned_df['POLICY6_CLOSE_SCHOOL'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('Close schools') >= 0 else 0) \n",
    "    cleaned_df['POLICY6_CLOSE_UNIVERSITY'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('Close universities') >= 0 else 0) \n",
    "    cleaned_df['POLICY6_CANCEL_LARGE_EVENT'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('Cancel large events') >= 0 else 0) \n",
    "    cleaned_df['POLICY6_OTHER'] = cleaned_df['POLICY6'].apply(lambda x: 1 if x.find('OTHER') >= 0 else 0) \n",
    "    cleaned_df.drop('POLICY6', axis = 1, inplace=True) \n",
    "\n",
    "if 'PERSONAL1' in cleaned_df.columns: \n",
    "    cleaned_df['PERSONAL1_WFH'] = cleaned_df['PERSONAL1'].apply(lambda x: 1 if x.find('Working from home') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL1_STAY_HOME'] = cleaned_df['PERSONAL1'].apply(lambda x: 1 if x.find('Staying home and only going out') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL1_DISTANCE_OLD_PEOPLE'] = cleaned_df['PERSONAL1'].apply(lambda x: 1 if x.find('older relatives') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL1_DISTANCE_FAM_MEMBERS'] = cleaned_df['PERSONAL1'].apply(lambda x: 1 if x.find('other family members') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL1_KEEP_KIDS_HOME1'] = cleaned_df['PERSONAL1'].apply(lambda x: 1 if x.find('because they are closed') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL1_KEEP_KIDS_HOME2'] = cleaned_df['PERSONAL1'].apply(lambda x: 1 if x.find('daycare voluntarily') >= 0 else 0) \n",
    "    cleaned_df.drop('PERSONAL1', axis = 1, inplace=True)     \n",
    "   \n",
    " \n",
    "if 'PERSONAL5' in cleaned_df.columns: \n",
    "    cleaned_df['PERSONAL5_WASH_HAND_OFTEN'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Wash hands more often') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_USE_HAND_SANITIZER'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Used hand sanitizer') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_TEST_COVID'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Sought out testing for Coronavirus') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_CHANGE_TRAVEL'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Changed travel plans') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_AVOID_CONTACT_OTHERS'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Avoided contact with others') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_AVOID_OLDER_ADULTS'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Avoiding contact with older adults') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_AVOID_GATHERING'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Avoided gatherings') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_SOUGHT_INFO'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Sought information on Coronavirus') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL5_SELF_QUARANTINED'] = cleaned_df['PERSONAL5'].apply(lambda x: 1 if x.find('Self-quarantined/isolated') >= 0 else 0) \n",
    "    cleaned_df.drop('PERSONAL5', axis = 1, inplace=True)  \n",
    "    \n",
    "if 'PERSONAL6' in cleaned_df.columns: \n",
    "    cleaned_df['PERSONAL6_SELF_COMPLICATION'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Experiencing serious complication') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_SOMEONE_COMPLICATION'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Someone I care about experiencing') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_NO_FOOD'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Not being able to put food') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_FEELING_ANXIOUS'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Feeling isolated, anxious') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_LACK_CARE'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Not being able to get needed care') >= 0 else 0)\n",
    "    cleaned_df['PERSONAL6_LACK_ACCESS'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Not being able to access care') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_KIDS_EDU'] =  cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('education') >= 0 else 0)   \n",
    "    cleaned_df['PERSONAL6_CANNOT_PAY_RENT'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('pay the rent') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_CANNOT_TAKE_CARE_OTHER'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('take care of family members') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_CANNOT_WORK'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Not being able to work') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_LONG_TERM_FINANCIAL'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Long term financial impacts') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL6_OTHER'] = cleaned_df['PERSONAL6'].apply(lambda x: 1 if x.find('Other,') >= 0 else 0) \n",
    "    cleaned_df.drop('PERSONAL6', axis = 1, inplace=True)  \n",
    "    \n",
    "if 'PERSONAL7' in cleaned_df.columns:      \n",
    "    cleaned_df['PERSONAL7_REDUCED_INCOME'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Reduced income') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL7_LOST_MY_JOB'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Lost my job') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL7_LOSS_CHILDCARE'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Loss of childcare') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL7_GETTING_FOOD'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Getting food') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL7_GETTING_SUPPLIES'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Getting supplies') >= 0 else 0)\n",
    "    cleaned_df['PERSONAL7_GETTING_MEDICATIONS'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Getting routine') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL7_TRANSPORTATION'] =  cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Transportation') >= 0 else 0)   \n",
    "    cleaned_df['PERSONAL7_ACCESS_HEALTHCARE'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Accessing healthcare') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL7_NONE'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('None of the above') >= 0 else 0) \n",
    "    cleaned_df['PERSONAL7_OTHER'] = cleaned_df['PERSONAL7'].apply(lambda x: 1 if x.find('Other,') >= 0 else 0) \n",
    "    cleaned_df.drop('PERSONAL7', axis = 1, inplace=True)\n",
    "\n",
    "if 'MEDIA3' in cleaned_df.columns:      \n",
    "    cleaned_df['MEDIA3_TV'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('TV') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_SOCIAL_MEDIA'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Social media') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_NEWSPAPER'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Newspapers') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_MAGAZINE'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Magazines') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_BLOG'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Blogs and podcasts') >= 0 else 0)\n",
    "    cleaned_df['MEDIA3_GOV_WEBSITE'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('government websites') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_HEALTH_WEBSITE'] =  cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('health department websites') >= 0 else 0)   \n",
    "    cleaned_df['MEDIA3_FRIENDS_FAMILY'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Friends or family members') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_HEALTH_PRACTITIONERS'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Health practitioners') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_HEALTH_RESEARCHER'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Health researchers') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_SCIEN_ARTICLES'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Scientific articles') >= 0 else 0) \n",
    "    cleaned_df['MEDIA3_OTHER'] = cleaned_df['MEDIA3'].apply(lambda x: 1 if x.find('Other,') >= 0 else 0)\n",
    "    cleaned_df.drop('MEDIA3', axis = 1, inplace=True)  \n",
    "    \n",
    "if 'MEDIA4' in cleaned_df.columns:      \n",
    "    cleaned_df['MEDIA4_TV'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('TV') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_SOCIAL_MEDIA'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Social media') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_NEWSPAPER'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Newspapers') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_MAGAZINE'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Magazines') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_BLOG'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Blogs and podcasts') >= 0 else 0)\n",
    "    cleaned_df['MEDIA4_GOV_WEBSITE'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('government websites') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_HEALTH_WEBSITE'] =  cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('health department websites') >= 0 else 0)   \n",
    "    cleaned_df['MEDIA4_FRIENDS_FAMILY'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Friends or family members') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_HEALTH_PRACTITIONERS'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Health practitioners') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_HEALTH_RESEARCHER'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Health researchers') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_SCIEN_ARTICLES'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Scientific articles') >= 0 else 0) \n",
    "    cleaned_df['MEDIA4_OTHER'] = cleaned_df['MEDIA4'].apply(lambda x: 1 if x.find('Other,') >= 0 else 0)\n",
    "    cleaned_df.drop('MEDIA4', axis = 1, inplace=True)  \n",
    "    \n",
    "if 'MEDIA8' in cleaned_df.columns:      \n",
    "    cleaned_df['MEDIA8_DIABETES'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('Diabetes') >= 0 else 0) \n",
    "    cleaned_df['MEDIA8_HEART_DISEASE'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('Heart disease') >= 0 else 0) \n",
    "    cleaned_df['MEDIA8_CANCER'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('Cancer') >= 0 else 0) \n",
    "    cleaned_df['MEDIA8_HIV'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('HIV') >= 0 else 0) \n",
    "    cleaned_df['MEDIA8_ASTHMA'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('Asthma') >= 0 else 0)\n",
    "    cleaned_df['MEDIA8_LUNG_DISEASE'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('lung Disease') >= 0 else 0)      \n",
    "    cleaned_df['MEDIA8_OTHER'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('Other') >= 0 else 0)\n",
    "    cleaned_df['MEDIA8_NONE'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('None') >= 0 else 0)\n",
    "    cleaned_df['MEDIA8_HYPERTENSION'] = cleaned_df['MEDIA8'].apply(lambda x: 1 if x.find('Hypertension') >= 0 else 0)    \n",
    "    cleaned_df.drop('MEDIA8', axis = 1, inplace=True)  \n",
    "    \n",
    "if 'MEDIA16' in cleaned_df.columns:  \n",
    "    cleaned_df['MEDIA16_SOUP_KITCHEN'] = cleaned_df['MEDIA16'].apply(lambda x: 1 if x.find('soup kitchen') >= 0 else 0) \n",
    "    cleaned_df['MEDIA16_SCHOOL'] = cleaned_df['MEDIA16'].apply(lambda x: 1 if x.find('school district') >= 0 else 0) \n",
    "    cleaned_df['MEDIA16_FOOD_STAMPS'] = cleaned_df['MEDIA16'].apply(lambda x: 1 if x.find('food stamps') >= 0 else 0) \n",
    "    cleaned_df['MEDIA16_COMMUNITY'] = cleaned_df['MEDIA16'].apply(lambda x: 1 if x.find('community food') >= 0 else 0) \n",
    "    cleaned_df['MEDIA16_FOOD_ASSIS'] = cleaned_df['MEDIA16'].apply(lambda x: 1 if x.find('food assistance') >= 0 else 0)\n",
    "    cleaned_df['MEDIA16_DONT_KNOW'] = cleaned_df['MEDIA16'].apply(lambda x: 1 if x.find(\"Don't know\") >= 0 else 0)  \n",
    "    cleaned_df.drop('MEDIA16', axis = 1, inplace=True)\n",
    "    \n",
    "if 'RUMORS1' in cleaned_df.columns:\n",
    "    cleaned_df['RUMORS1_ACCIDENTLY_ESCAPED'] = cleaned_df['RUMORS1'].apply(lambda x: 1 if x.find('accidently escaped') >= 0 else 0) \n",
    "    cleaned_df['RUMORS1_PURPOSEFULLY_LEAKED'] = cleaned_df['RUMORS1'].apply(lambda x: 1 if x.find('purposefully leaked') >= 0 else 0) \n",
    "    cleaned_df['RUMORS1_USARMY_BROUGHT'] = cleaned_df['RUMORS1'].apply(lambda x: 1 if x.find('The US army brought') >= 0 else 0) \n",
    "    cleaned_df['RUMORS1_BAT_TO_HUMANS'] = cleaned_df['RUMORS1'].apply(lambda x: 1 if x.find('from bat to humans') >= 0 else 0)\n",
    "    cleaned_df['RUMORS1_BAT_ANIMAL_HUMANS'] = cleaned_df['RUMORS1'].apply(lambda x: 1 if x.find('from bats to another animal') >= 0 else 0) \n",
    "    cleaned_df['RUMORS1_WET_MARKET'] = cleaned_df['RUMORS1'].apply(lambda x: 1 if x.find('It was introduced in a') >= 0 else 0) \n",
    "    cleaned_df['RUMORS1_OTHER'] = cleaned_df['RUMORS1'].apply(lambda x: 1 if x.find('Other,') >= 0 else 0) \n",
    "    cleaned_df.drop('RUMORS1', axis = 1, inplace=True)\n",
    "\n",
    "listOfStudents = ['Student', 'student', 'am a student', 'I am a full time student.', 'Unemployed  student']    \n",
    "listOfUnemployed = ['Unemployed', 'Unemployed, unable to work', 'Unemployed, looking for for', \n",
    "                'unemployed contractor', 'unemployed', 'Unemployed/Disabled', \n",
    "                 'Unemployed, unable to work', 'Unemployed  student']\n",
    "\n",
    "listOfRetired = ['Retired', 'retired', 'retired; some house sitting', 'Retired postal employee',\n",
    "                'retired loan officer', 'retired disabled', 'Retired librarian', \n",
    "                 'Retired. Fulltime Caregiver for husband.', 'retired health care',\n",
    "                'retired educator', 'retired and happy', 'I am retired.', 'Retired counselor',\n",
    "                'RETIRED  PREVIOUS WAS AN ACCOUNTANT', 'retired teacher', 'Retired Dental Hygienist',\n",
    "                'retired administrative assistant', 'fully retired', 'Retired RN',\n",
    "                'Retired. Hoping to work part time.', 'Retired Accountant', \n",
    "                 'Retired veterinary technician', 'retired chemist']\n",
    "\n",
    "listOfJobs = [\"Manufacturing \",\"Account\",\"Childcare \",\"Transportation \",\"paralegal \",\n",
    "              \"Rehab assistant \",\"Mechanic \",\"public health\",\"education \",\"driving \",\"Analysist\",\n",
    "              \"Transportation \",\"sales\",\"manager \",\"Director \",\"Caregiver \",\n",
    "              \"Customer service representative\",\"Healthcare \",\"Media Blogger \",\"clergy\",\n",
    "              \"Preschool teacher \",\"freelancer\",\"Hr\",\"Office\",\"Temporary Labor\",\"Courier \", \n",
    "              \"It Director\",\"Blogger\",\"Graphic designer/photographer \",\"Self employed\",\"Theme Parks \",\n",
    "              \"IT\",\"Nurse\",\"Business owner\",\"salesman\",\"Technical\",\"Deli associate\",\"Sales\",\n",
    "              \"Finsnce\",\"dont work\",\"Office staff at a school\",\"Bar manager\",\"Farmer\",\"Geological \",\n",
    "              \"worker\",\"i have an online retail store\",\"Decorator \",\"Management\",\"Supervisor \",\n",
    "              \"Director \",\"Finance\",\"manager \",\"manager\",\"Business \",\"Engineer \",\"it\",\"Teacher\",\n",
    "              \"Retail merchandiser \",\"Beautician\",\"Sakes manager\",\"clerk\",\"Nutritionist \",\"Director\",\n",
    "              \"Teacher\",\"Information technology manager\",\"Business man\",\"Journalist\",\n",
    "              \"Customer service specialist\",\"Data analyst \",\"IT professional\",\"housekeeper\",\"it\",\n",
    "              \"Financial services\",\"Team lead\",\"Social worker\",\"Self Employed\",\"Health \",\"Daycare\",\n",
    "              \"Retail \",\"Physical therapy\",\"Manager\",\"Plumber \",\"Retail \",\"Barista\",\n",
    "              \"MANUFACTURA\",\"Engineering\",\"Office coordinator \",\"Seller\",\"Bank\",\"Accounting\",\n",
    "              \"Cannabis trimmer\",\"it programmer\",\"Manufacturing\",\"It officer\",\"Construction\",\n",
    "              \"Farmer\",\"Manager\",\"engineer\",\"IT\",\"Computer \",\"Legal\",\"Retail \",\n",
    "              \"government transcriptionist\",\"Engineering\",\"Healthcare\",\"Accounting\",\"Manufacturing\",\n",
    "              \"Retail Associate \",\"Manager\",\"Information technology officer\",\"Art museum worker \",\n",
    "              \"Cashier \",\"Computer engineer\",\"Marketing \",\"Stay at home mom\",\"Constrution\",\"Banker\",\n",
    "              \"Legal assistant \",\"Educator\",\"Homemaker\",\"Banks \",\"Banking \",\"Manager\",\"Doctor\",\n",
    "              \"Artist \",\"Typist \",\"Banking\",\"Engineer\",\"Cashier\",\"Plumber\",\"nurise\",\"Security\",\n",
    "              \"Financial manager\",\"Sales and Marketing Manager at a retail store.\",\"Warehousing \",\n",
    "              \"It/communications \",\"Computer analyst\",\"nursing home\",\"Chef\",\"Retail\",\"Housekeeper\",\n",
    "              \"Homemaker \",\"owner of a small business \",\"Information technology\",\"Telecommunications\",\n",
    "              \"Ecommerce seller of handmade jewelry\",\"Teacher \",\"Financial analyst \",\"An accountant\",\n",
    "              \"Military personnel\",\"sales clerk\",\"Teaching \",\"Transportation \",\"Marketing \",\n",
    "              \"Grocery \",\"IT\",\"Information technologist\",\"eCommerce Associate\",\"Accounting\",\n",
    "              \"Information technology\",\"Medical lab Scientist\",\"Community organizer \",\n",
    "              \"A financial expert\",\"Dating service Director\",\"Nurse\",\"constructor\",\"Accountanting\",\n",
    "              \"Healthcare\",\"Information Technology \",\"Health Aide\",\"Security \",\n",
    "              \"computer software and hardware and development\",\"Flea market\",\"Construction\",\n",
    "              \"Consultation \",\"Banker\",\"Educator \",\"computer software\",\"Software developer \",\n",
    "              \"Plumber \",\"Painter\",\"Banker\",\"Engineer\",\"IT tecnology\",\"IT\",\"Information technology\",\n",
    "              \"Banking and finance \",\"Typist\",\"engineer\",\"Software Engineer\",\"Trainer \",\"Direto\",\n",
    "              \"Manufacturer\",\"finance\",\"Financial services \",\"Diretor \",\"Doctore\",\n",
    "              \"alchohol distribution\",\"Accountant \",\"Purchasing director\",\"Information technology\",\n",
    "              \"Manager\",\"Director of IT\",\"Officer in charge\",\"Automotive\",\"Bank branch manager\",\n",
    "              \"Director\",\"Financial institution \",\"Technology\",\"Finance \",\"Prep Cook\",\"Developer \",\n",
    "              \"Accountant \",\"Manager of tech\",\"Software developer\",\"Financial advisor\",\"IT Manager \",\n",
    "              \"Xray technologist\",\"Insurance\",\"IT director\",\"teacher\",\"Insurance\",\n",
    "              \"Department of defense\",\"Academian \",\"Telecommunication.\",\"Construction\",\n",
    "              \"Agriculture \",\"Tailoring \",\"Clerical Staff\",\"Information technology / IT\",\n",
    "              \"Information Technology \",\"Retai \",\"ADMIN\",\"Construction\",\"IT\",\"Permanently disabled \",\n",
    "              \"Medical Transcriptionist\",\"Sales\",\"Nurse aide\",\"Self-Employed\",\"Food Manager\",\n",
    "              \"Civil servant\",\"Caregiver\",\"IT MANAGER\",\"Data analyst\",\"Disabled, was a chemist.\",\n",
    "              \"Analyst\",\"ceo\",\"Cheif information officer \",\"IT Instructor\",\"Sales\",\"Quality control \",\n",
    "              \"Retired\",\"IT\",\"Any work that involves money \",\"Financial services\",\"manager\",\n",
    "              \"retail marketing \",\"Arts and equestrian and recreation events \",\"Manager\",\n",
    "              \"Insurance \",\"quality assurance inspection technician\",\"Graphic Design\",\n",
    "              \"Student transportation\",\"Healthcare assistant\",\"developer\",\"Legal assistant\",\"ASPCA\",\n",
    "              \"Nurse\",\"Accounting\",\"Admin Assistant\",\"Retail Associate\",\"Manager\",\"cashier\",\n",
    "              \"Consulting\",\"HEALTHCARE\",\"Education industry \",\"dishwasher\",\"Financial Analyst \",              \n",
    "              \"librarian\",\"Receptionist\",\"caregiver\",\"Realtor\",\"Nurse \",\"Accountant\",\n",
    "              \"program manager\",\"pbx operator\",\"staffing\",\"Banker\",\"Network Engineer\",\"Retail\",\n",
    "              \"writer\",\"Business\",\"Information technology \",\"Dumcuter\",\"CPA\",\"transcription\",\n",
    "              \"Employee\",\"Engineer\",\"Construction\",\"Distribution\",\"Tax Analyst\",\n",
    "              \"Information technology\",\"Information technologist \",\"Engineer \",\"Employed\",\"Cook\",\n",
    "              \"Senior Manager\",\"Teacher\",\"IT\",\"Engineer\",\"Information Technologist\",\"Caretakwr\",\n",
    "              \"construction\",\"New Custom Home Painter\",\"Retired. Fulltime Caregiver for husband.\",\n",
    "              \"retired health care\",\"retired\",\"Banking and finance\",\"Information and technology \",             \n",
    "              \"Financial analysts manager \",\"Healthcare\",\"IT/Computer services\",\"Insurance\",\n",
    "              \"Real estate manager\",\"Automotive \",\"Health worker\",\"It\",\n",
    "              \" work in the HR department\",\"Education\",\"sales\",\"IT expert\",\"student\",\n",
    "              \"Pharmacy technician \",\"Business owner\",\"Information technology\",\"Insurance\",\n",
    "              \"Manufacturer\",\"Football player \",\"general contractor\",\"homemaker\",\"management \",\n",
    "              \"Real Estate\",\"healthcare\",\"Information Technology\",\"consultant\",\"Educator\",\n",
    "              \"Office assistant\",\"Plumber \",\"Surgeon \",\"healthcare\",\"teacher\",\"MACHINING\",\"Designer\",\n",
    "              \"Retail sales\",\"school cafe worker\",\"Student\",\"Student \",\"homemaker\",\"Social services\",\n",
    "              \"Information Technology\",\"Construction \",\"Farmer \",\"Cashier\",\"i work\",\n",
    "              \"Director Loyalty \",\"Accounting \",\"education\",\"Teacher\",\"Childcare \",\"Pet care\",\n",
    "              \"Director\",\"I.T Manager\",\"Technician\",\"manager of a private company\",\n",
    "              \"I work in the Information Technology.\",\"Warehouse Clerk\",\"Customer Service Rep\",\n",
    "              \"IT\",\"Management consultant\",\"Retired\",\"Senior Computer Systems Engineer\",\"Advise\",\n",
    "              \"Market research\",\"Cook\",\"Healthcare \",\"Order Entry(Temporary/Seasonal).\",\"cashier\",\n",
    "              \"Healthcare \",\"Civil service \",\"Landscaping helper\",\"student\",\"Healthcare\",\"driver\",\n",
    "              \"merchandiser\",\"freelancer\",\"Waretired private t duty hospice care provider\",\n",
    "              \"IT Director\",\"Barista\",\"The best experience \",\"Educator \",\"Nurse\",\"Dealer\",\n",
    "              \"Geological \",\"Director\",\"Manager\",\"Accountant\",\"Construction\",\"Data analysis \",\n",
    "              \"Telecommunications \",\"saller\",\"assistant\",\"Manager at IT company\",\"farmer\",\n",
    "              \"A writer.\",\"Financial Advisor\",\"Retail\",\"Manager\",\"Programmer\",\"Financial officer\",\n",
    "              \"Information technology \",\"Telecommunications director \",\"Warehouse\",\"Technologist\",\n",
    "              \"Teacher\",\"IT manager\",\"banker\",\"Constructor \",\"Blogger and advertiser\",\"Engineer \",\n",
    "              \"Retail associate and student\",\"Database Analyst\",\"medical  administrator\",\n",
    "              \"construction/carpenter\",\"Information technologist\",\"Typist\",\"Engineer\",\n",
    "              \"Banking and finance \",\"Pampered chef Rep\",\"2\",\"Manager\",\"retired teacher\",\n",
    "              \"Social Worker\",\"information tehnology\",\"IT \",\"Retired Dental Hygienist \",\"Banker\",\n",
    "              \"IT\",\"Electrical enginner\",\"HOMEMAKER\",\"freelance writer\",\"IT  analyst \",\"Consultant\",\n",
    "              \"nurse assistant\",\"Hotel Manager\",\"IT Professional\",\"Inventory Management\",\"Farmer\",\n",
    "              \"Teacher\",\"IT\",\"Banking and financial services \",\"social worker\",\"Good great \",\n",
    "              \"Manager\",\"Manager of a company\",\"Banker\",\"employed full time\",\"Farmer\",\"Nurse\",\n",
    "              \"Realtor \",\"General construction\",\"BANKING FINACES\",\"Administrator\",\"Encanador\",\n",
    "              \"Factory worker\",\"Clerical \",\"Retail\",\"Typist\",\"Freelance\",\"adasdsadsa\",\n",
    "              \"InformationTechnology\",\"CEO\",\"management \",\"3\",\"IT\",\"Administrative \",\n",
    "              \"Accountant\",\"Lawery\",\"Doctor\",\"Restaurant \",\"Administrator\",\"healthcare aide\",\n",
    "              \"Financial advisor\",\"Engineer\",\"independent contractor\",\"plumber\",\"carpenter\",\n",
    "              \"Tech manufacturer\",\"Accountant \",\"Banking\",\"Finances\",\"Financial Director\",\n",
    "              \"Business analyst\",\"Paralegal \",\"Financial director\",\"Scientist \",\"Education\",\n",
    "              \"Docter\",\"Education\",\"Hair stylist \",\"Executive\",\"Financial controller \",\n",
    "              \"Administrator\",\"Manager\",\"Businessman\",\"Nurse \",\"Banking and finance\",\n",
    "              \"Parish administrator \",\"Financial advisor\",\"Banker\",\"retail/whole sale\",\n",
    "              \"I like\",\"support specialist\",\"IT Expert\",\"Computer engineer\",\"Consultant\",\n",
    "              \"Accountant\",\"PHYSICIAN\",\"Healthcare \",\"Information technologist\",\n",
    "              \"Managing director\",\"microbiologist\",\"Medical Doctor\",\"Sweet compsny \",\"dentist\",\n",
    "              \"Financial specialist\",\"Farmer\",\"GIG Delivery for Postmates\",\"Business administrator\",\n",
    "              \"HVAC \",\"Manager\",\"Civil Engineer\",\"Health care\",\"Tecnology\",\"Healthcare\",\"Doctor\",\n",
    "              \"Teacher\",\"Middle manager\",\"Insurance\",\"Designer \",\"semi retired retail\",\"Associate \",\n",
    "              \"HR\",\"Artist \",\"office management \",\"Retail \",\"Farmer\",\"Arts\",\n",
    "              \"Customer Service Representative\",\"Construction\",\"Healthcare\",\"Contractor\",\n",
    "              \"Director for a boys and girls club \",\"0\",\"Manager \",\"manager\",\"Paint\",\"IT\",\n",
    "              \"manager\",\"Package handler \",\"Substitute teacher\",\"Art\",\"Customer service \",\n",
    "              \"Freelance Author\",\"Child Care\",\"Good\",\"Foreman \",\"Receptionist \",\"Engineer\",\n",
    "              \"Data entry clerk \",\"Photographer \",\"Sales\",\"healthcare \",\"Education\",\"Car detail\",\n",
    "              \"Journalism\",\"Self emoyeed\",\"Counselor \",\"Transportation \",\"Home maker\",\"Walmart \",\n",
    "              \"Tefinish furniture\",\"Information technology\",\"I work at Mcdonalds fast food \",\n",
    "              \"Management\",\"Martial arts trainer\",\"Retail\",\"Information technology\",\"manager\",\n",
    "              \"Emwd\",\"Retired veterinary technician\",\"Banker\",\"Education\",\"project manager\",\n",
    "              \"Creative designer \",\"information technology\",\"hr mgr\",\"Crocheter\",\"Plumber\",\"Banker \",\n",
    "              \"Entrepreneur in the mental health space\",\"Consulting\",\"IT manager\",\"Secretary\",\n",
    "              \"Consultant\",\"Construction\",\"Photography\",\"Tech\",\"plumber\",\"Cleaner\",\n",
    "              \"Proffesional business \",\"Hacker\",\"Famous rapper \",\"Nurse aide \",\"Retail \",\n",
    "              \"Dogwalker\",\"Health care\",\"manager\",\"Encoder\",\"Information technology\",\"Consultant \",\n",
    "              \"Education\",\"Customer service \",\"IT\",\"food service\",\"Information  technology\",\n",
    "              \"Data specialist \",\"IT\",\"IT MANAGER\",\"Food service Worker \",\"information technology\",\n",
    "              \"Education\",\"manufacture\",\"That could be very good and information technology.\",\n",
    "              \"medical assistant\",\"Food server\",\"Home health\",\"Estate clearances\",\"It Consult\",\n",
    "              \"Interior decorator \",\"Tech\",\"Restaurant Manager\",\"manufacturing\",\"Intern\",\n",
    "              \"Receptionist\",\"sales manager\",\"Engineering\",\"retail sales \",\"Construction\",\n",
    "              \"State/government\",\"Banker \",\"General contractor \",\"Banker\",\"Chuef technology officer\",\n",
    "              \"Marketer\",\"Information Technologist consultant\",\"Electrician\",\"Engineering \",\"IT\",\n",
    "              \"Manager \",\"Chef\",\"Real estate \",\"Engineer\",\"Security Officer\",\"Information Technology\",\n",
    "              \"Information technology\",\"IT\",\"Freelance \",\"Gnome\",\"Banker\",\"Part-time worker \",\n",
    "              \"Sinticts\",\"Information Technology \",\"Plumer typist\",\"Software engineer\",\"Advertising\",\n",
    "              \"Financial institution \",\"Sales\",\"IT professional \",\"Teacher\",\"Economist\",\n",
    "              \"i working banking and financilal project\",\"Programmer\",\"Banking/finance \",\n",
    "              \"IT related\",\"Construction\",\"House cleaning\",\"Mechanic \",\"Teacher\",\"private job\",\n",
    "              \"Chemist\",\"Business man\",\"Make eoeneoemekem\",\"Health care \",\"Management\",\n",
    "              \"medical records clerk\",\"Finance\",\"clerk\",\"Information Technology\",\"Chef\",\"Teacher\",\n",
    "              \"Finace\",\"Auto detailing\",\"Director\",\"Business services business owner\",\"entrey level\",\n",
    "              \"IT specialist\",\"stay at home parent\",\"Homemaker \",\"teacher..4th grader teacher\",\n",
    "              \"Music Engineer\",\"I. T\",\"Proffesional business \",\"Accountant\",\"manager\",\n",
    "              \"times plumber\",\"construction\",\"Plumber aSsiant \",\"Disabled \",\"Accountant \",\n",
    "              \"Procurement Analyst\",\"Modeling \",\"Administrator\",\"Financial advisor\",\"Finance\",\n",
    "              \"Information technology\",\"Doctor\",\"real estate\",\"Self employed\",\"graphic design\",\n",
    "              \"Director\",\"Programmer\",\"Information technology\",\"information technology \",\n",
    "              \"Accounting\",\"Kitchen worker\",\"Plumer\",\"Insurance \"]\n",
    " \n",
    "listOfAll= listOfJobs + listOfUnemployed + listOfRetired + listOfStudents\n",
    "\n",
    "if 'DEMOGRAPHIC1' in cleaned_df.columns:\n",
    "    \n",
    "    cleaned_df['DEMOGRAPHIC1_EMPLOYED']=cleaned_df['DEMOGRAPHIC1'].apply(lambda x: 1 if x in listOfJobs else 0) \n",
    "    cleaned_df['DEMOGRAPHIC1_UNEMPLOYED']=cleaned_df['DEMOGRAPHIC1'].apply(lambda x: 1 if x in listOfUnemployed else 0) \n",
    "    cleaned_df['DEMOGRAPHIC1_RETIRED']=cleaned_df['DEMOGRAPHIC1'].apply(lambda x: 1 if x in listOfRetired else 0)\n",
    "    cleaned_df['DEMOGRAPHIC1_STUDENTS']=cleaned_df['DEMOGRAPHIC1'].apply(lambda x: 1 if x in listOfStudents else 0) \n",
    "    cleaned_df['DEMOGRAPHIC1_OTHER']=cleaned_df['DEMOGRAPHIC1'].apply(lambda x: 0 if x in listOfAll else 1) \n",
    "    cleaned_df.drop('DEMOGRAPHIC1', axis = 1, inplace=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7b827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39d19a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in column :  HR11 , missing rows:  2 , missing percentage:  0.16515276630883566 %\n",
      "Missing Values in column :  COVID_HR1 , missing rows:  485 , missing percentage:  40.04954582989265 %\n",
      "Missing Values in column :  COVID_HR2 , missing rows:  404 , missing percentage:  33.360858794384804 %\n",
      "Missing Values in column :  IDEOLOGY4 , missing rows:  323 , missing percentage:  26.67217175887696 %\n"
     ]
    }
   ],
   "source": [
    " #Check for missing values, fill nan\n",
    "for (columnName, columnData) in cleaned_df.iteritems(): \n",
    "    missingCount = columnData.isnull().sum() \n",
    "    percentage = (missingCount/totalRows)*100 \n",
    "    #print(percentage)\n",
    "    if missingCount > 0:        \n",
    "        print(\"Missing Values in column : \", columnName, \", missing rows: \", \n",
    "            missingCount, \", missing percentage: \",  percentage, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41a57eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " #Fill missing values\n",
    "cleaned_df.fillna('NA', inplace=True)\n",
    "try:\n",
    "    cleaned_df.to_csv(\"cleaned_df.csv\") \n",
    "except PermissionError as prr:\n",
    "    print(\"Cannot save file if you have it open in Windows \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1cad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d12b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2222715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
